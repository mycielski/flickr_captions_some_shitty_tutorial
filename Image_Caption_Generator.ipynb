{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Building Image Caption Generator Using LSTM***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "KibAubOYjKbs"
   },
   "source": [
    "## ***1-Importing packages and Libraries***"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pc-QjsZmjVEw",
    "outputId": "97ea745e-abb7-49d8-94fa-8e015f5c8ab7",
    "ExecuteTime": {
     "end_time": "2025-06-01T23:25:51.466195Z",
     "start_time": "2025-06-01T23:25:48.106283Z"
    }
   },
   "source": [
    "from tensorflow.keras.layers import Input , Dense , Embedding , LSTM , Dropout , add\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.applications.xception import Xception\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from nltk.stem.snowball import stopwords\n",
    "from tensorflow.keras.models import Model\n",
    "from matplotlib import image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_text as text\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import seaborn\n",
    "import random\n",
    "import pickle\n",
    "#import spacy\n",
    "import nltk\n",
    "import gzip\n",
    "import cv2\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "#nltk.download(\"all\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tmyciels/Desktop/image-caption-generator/.venv/lib/python3.11/site-packages/tensorflow_hub/__init__.py:61: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import parse_version\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T23:25:51.494779Z",
     "start_time": "2025-06-01T23:25:51.492936Z"
    }
   },
   "cell_type": "code",
   "source": "# set up tensorflow to use apple mps\n",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UXw7m0PPRfrr",
    "ExecuteTime": {
     "end_time": "2025-06-01T23:25:51.502997Z",
     "start_time": "2025-06-01T23:25:51.500984Z"
    }
   },
   "source": "# !uv run python -m spacy download en_core_web_md",
   "outputs": [],
   "execution_count": 3
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "q1T-MjsoNvyn"
   },
   "source": [
    "## ***2-Preprocessing text***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "BOEDsJeHV1-C"
   },
   "source": [
    "* Punctuation removal\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FmPc4-EWVzEN",
    "ExecuteTime": {
     "end_time": "2025-06-01T23:25:51.509950Z",
     "start_time": "2025-06-01T23:25:51.508159Z"
    }
   },
   "source": [
    "def remove_punc(text) :\n",
    "  return re.sub(r'[^\\w\\s]','',text)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "DaltQbK3N8dE"
   },
   "source": [
    "* Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Z0suY6orEkw5",
    "ExecuteTime": {
     "end_time": "2025-06-01T23:25:51.517715Z",
     "start_time": "2025-06-01T23:25:51.516038Z"
    }
   },
   "source": [
    "def to_lower_case(text) :\n",
    "  return text.lower()"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "R_hENLZ1b81M"
   },
   "source": [
    "* Removing Stopwords"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T23:25:51.716341Z",
     "start_time": "2025-06-01T23:25:51.524048Z"
    }
   },
   "cell_type": "code",
   "source": "nltk.download('stopwords')",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tmyciels/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oJuM4OKwWpwc",
    "ExecuteTime": {
     "end_time": "2025-06-01T23:25:51.731590Z",
     "start_time": "2025-06-01T23:25:51.728540Z"
    }
   },
   "source": [
    "stopwords_list = stopwords.words('english')\n",
    "def remove_stopwords(text) :\n",
    "  text_words = [word for word in text.split() if ((word not in stopwords_list) and (len(word) > 2))]\n",
    "  text = \" \".join(text_words)\n",
    "  return text"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "gW_RHCqCeZFL"
   },
   "source": [
    "* Removing numbers"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "v3-HvWbJeZRa",
    "ExecuteTime": {
     "end_time": "2025-06-01T23:25:51.753874Z",
     "start_time": "2025-06-01T23:25:51.752042Z"
    }
   },
   "source": [
    "def remove_numbers(text) :\n",
    "  return re.sub(r'[0-9]','',text)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "pr9O3w4Df34d"
   },
   "source": [
    "* Removing multiple whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Jm2WI7xff_9s",
    "ExecuteTime": {
     "end_time": "2025-06-01T23:25:51.772969Z",
     "start_time": "2025-06-01T23:25:51.771Z"
    }
   },
   "source": [
    "def remove_multiple_spaces(text) :\n",
    "  return re.sub(' +',' ',text).strip()"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "u0vBytsE5Fdj",
    "ExecuteTime": {
     "end_time": "2025-06-01T23:25:51.785546Z",
     "start_time": "2025-06-01T23:25:51.782974Z"
    }
   },
   "source": [
    "# gathering all the text cleaning steps in one function\n",
    "def clean_text(text) :\n",
    "  text = remove_punc(text)\n",
    "  text = to_lower_case(text)\n",
    "  text = remove_stopwords(text)\n",
    "  text = remove_numbers(text)\n",
    "  text = remove_multiple_spaces(text)\n",
    "  return text"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "dSsfnxXFImuX"
   },
   "source": [
    "## ***3-Preprocessing data***"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lERV0gHn5FgF",
    "ExecuteTime": {
     "end_time": "2025-06-01T23:25:51.794912Z",
     "start_time": "2025-06-01T23:25:51.792812Z"
    }
   },
   "source": [
    "def read_file(path) : \n",
    "  with open(path, 'r') as file :\n",
    "    return file.read().split('\\n')"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9m1r6v4NDtqf",
    "ExecuteTime": {
     "end_time": "2025-06-01T23:25:51.806181Z",
     "start_time": "2025-06-01T23:25:51.803479Z"
    }
   },
   "source": [
    "# converting image_captions data into dict where keys = images and values = captions \n",
    "def get_data_dictionary(data) :\n",
    "  descriptions = {}\n",
    "  for line in data :\n",
    "    image_name , caption = line.split('\\t')\n",
    "    if image_name[:-2] in descriptions.keys() :\n",
    "      descriptions[image_name[:-2]].append(caption)\n",
    "    else :\n",
    "      descriptions[image_name[:-2]] = [caption]\n",
    "  return descriptions"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "g1pr3IM4RehI",
    "ExecuteTime": {
     "end_time": "2025-06-01T23:25:51.816237Z",
     "start_time": "2025-06-01T23:25:51.814462Z"
    }
   },
   "source": [
    "# using predefined text preprocessing functions to clean the captions text\n",
    "def clean_captions(descriptions) :\n",
    "  for image in descriptions.keys() :\n",
    "    for index , caption in enumerate(descriptions[image]) :\n",
    "      descriptions[image][index] = clean_text(caption)\n",
    "  return descriptions"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UBxfiJHXT9Sh",
    "ExecuteTime": {
     "end_time": "2025-06-01T23:25:51.827442Z",
     "start_time": "2025-06-01T23:25:51.824543Z"
    }
   },
   "source": [
    "# writing down data dictionary into external file\n",
    "def write_file(path,data) :\n",
    "  lines = []\n",
    "  for image in data.keys() :\n",
    "    for caption in data[image] :\n",
    "      lines.append(image+'\\t'+caption)\n",
    "  lines = '\\n'.join(lines)\n",
    "  with open(path,'w') as file :\n",
    "    file.write(lines)"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "x4zReDOMaiiO",
    "ExecuteTime": {
     "end_time": "2025-06-01T23:25:53.474601Z",
     "start_time": "2025-06-01T23:25:51.837576Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "data = read_file(Path.cwd() / \"flicker_30k\" / \"Flickr8k.token.txt\")\n",
    "descriptions = get_data_dictionary(data)\n",
    "descriptions = clean_captions(descriptions)\n",
    "path = Path.cwd() / \"cleaned_data.txt\"\n",
    "write_file(path,descriptions)"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "7oUW0C8QTi1b"
   },
   "source": [
    "## ***4-Extracting Images Features***"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZVKM0hgfTnOo",
    "outputId": "430cef1c-5bfa-4a7c-b288-e1092f8d07bf",
    "ExecuteTime": {
     "end_time": "2025-06-01T23:25:55.447467Z",
     "start_time": "2025-06-01T23:25:53.482141Z"
    }
   },
   "source": [
    "# importing VGG16 model without the output layer\n",
    "features_extractor = VGG16()\n",
    "features_extractor = Model(inputs = features_extractor.inputs  , outputs =  features_extractor.layers[-2].output)\n",
    "features_extractor.summary()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-02 01:25:53.484606: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3 Pro\n",
      "2025-06-02 01:25:53.484642: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 18.00 GB\n",
      "2025-06-02 01:25:53.484646: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 6.00 GB\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1748820353.484954  631959 pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1748820353.485149  631959 pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"functional\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001B[38;5;33mInputLayer\u001B[0m)        │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m224\u001B[0m, \u001B[38;5;34m224\u001B[0m, \u001B[38;5;34m3\u001B[0m)    │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block1_conv1 (\u001B[38;5;33mConv2D\u001B[0m)           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m224\u001B[0m, \u001B[38;5;34m224\u001B[0m, \u001B[38;5;34m64\u001B[0m)   │         \u001B[38;5;34m1,792\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block1_conv2 (\u001B[38;5;33mConv2D\u001B[0m)           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m224\u001B[0m, \u001B[38;5;34m224\u001B[0m, \u001B[38;5;34m64\u001B[0m)   │        \u001B[38;5;34m36,928\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block1_pool (\u001B[38;5;33mMaxPooling2D\u001B[0m)      │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m112\u001B[0m, \u001B[38;5;34m112\u001B[0m, \u001B[38;5;34m64\u001B[0m)   │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block2_conv1 (\u001B[38;5;33mConv2D\u001B[0m)           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m112\u001B[0m, \u001B[38;5;34m112\u001B[0m, \u001B[38;5;34m128\u001B[0m)  │        \u001B[38;5;34m73,856\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block2_conv2 (\u001B[38;5;33mConv2D\u001B[0m)           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m112\u001B[0m, \u001B[38;5;34m112\u001B[0m, \u001B[38;5;34m128\u001B[0m)  │       \u001B[38;5;34m147,584\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block2_pool (\u001B[38;5;33mMaxPooling2D\u001B[0m)      │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m56\u001B[0m, \u001B[38;5;34m56\u001B[0m, \u001B[38;5;34m128\u001B[0m)    │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block3_conv1 (\u001B[38;5;33mConv2D\u001B[0m)           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m56\u001B[0m, \u001B[38;5;34m56\u001B[0m, \u001B[38;5;34m256\u001B[0m)    │       \u001B[38;5;34m295,168\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block3_conv2 (\u001B[38;5;33mConv2D\u001B[0m)           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m56\u001B[0m, \u001B[38;5;34m56\u001B[0m, \u001B[38;5;34m256\u001B[0m)    │       \u001B[38;5;34m590,080\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block3_conv3 (\u001B[38;5;33mConv2D\u001B[0m)           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m56\u001B[0m, \u001B[38;5;34m56\u001B[0m, \u001B[38;5;34m256\u001B[0m)    │       \u001B[38;5;34m590,080\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block3_pool (\u001B[38;5;33mMaxPooling2D\u001B[0m)      │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m28\u001B[0m, \u001B[38;5;34m28\u001B[0m, \u001B[38;5;34m256\u001B[0m)    │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block4_conv1 (\u001B[38;5;33mConv2D\u001B[0m)           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m28\u001B[0m, \u001B[38;5;34m28\u001B[0m, \u001B[38;5;34m512\u001B[0m)    │     \u001B[38;5;34m1,180,160\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block4_conv2 (\u001B[38;5;33mConv2D\u001B[0m)           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m28\u001B[0m, \u001B[38;5;34m28\u001B[0m, \u001B[38;5;34m512\u001B[0m)    │     \u001B[38;5;34m2,359,808\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block4_conv3 (\u001B[38;5;33mConv2D\u001B[0m)           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m28\u001B[0m, \u001B[38;5;34m28\u001B[0m, \u001B[38;5;34m512\u001B[0m)    │     \u001B[38;5;34m2,359,808\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block4_pool (\u001B[38;5;33mMaxPooling2D\u001B[0m)      │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m14\u001B[0m, \u001B[38;5;34m14\u001B[0m, \u001B[38;5;34m512\u001B[0m)    │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block5_conv1 (\u001B[38;5;33mConv2D\u001B[0m)           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m14\u001B[0m, \u001B[38;5;34m14\u001B[0m, \u001B[38;5;34m512\u001B[0m)    │     \u001B[38;5;34m2,359,808\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block5_conv2 (\u001B[38;5;33mConv2D\u001B[0m)           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m14\u001B[0m, \u001B[38;5;34m14\u001B[0m, \u001B[38;5;34m512\u001B[0m)    │     \u001B[38;5;34m2,359,808\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block5_conv3 (\u001B[38;5;33mConv2D\u001B[0m)           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m14\u001B[0m, \u001B[38;5;34m14\u001B[0m, \u001B[38;5;34m512\u001B[0m)    │     \u001B[38;5;34m2,359,808\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block5_pool (\u001B[38;5;33mMaxPooling2D\u001B[0m)      │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m7\u001B[0m, \u001B[38;5;34m7\u001B[0m, \u001B[38;5;34m512\u001B[0m)      │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001B[38;5;33mFlatten\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m25088\u001B[0m)          │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ fc1 (\u001B[38;5;33mDense\u001B[0m)                     │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m4096\u001B[0m)           │   \u001B[38;5;34m102,764,544\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ fc2 (\u001B[38;5;33mDense\u001B[0m)                     │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m4096\u001B[0m)           │    \u001B[38;5;34m16,781,312\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block1_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block1_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block1_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block2_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block2_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │       <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block2_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block3_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block3_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block3_conv3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block3_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block4_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,180,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block4_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block4_conv3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block4_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block5_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block5_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block5_conv3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block5_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25088</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ fc1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)           │   <span style=\"color: #00af00; text-decoration-color: #00af00\">102,764,544</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ fc2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)           │    <span style=\"color: #00af00; text-decoration-color: #00af00\">16,781,312</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m134,260,544\u001B[0m (512.16 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">134,260,544</span> (512.16 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m134,260,544\u001B[0m (512.16 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">134,260,544</span> (512.16 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uPEXfWYxUh9P",
    "ExecuteTime": {
     "end_time": "2025-06-01T23:25:55.492038Z",
     "start_time": "2025-06-01T23:25:55.475951Z"
    }
   },
   "source": [
    "images_path = Path.cwd() / \"flickr30k_images\" / \"flickr30k_images\"\n",
    "images_names = os.listdir(images_path)"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T23:25:55.515949Z",
     "start_time": "2025-06-01T23:25:55.498776Z"
    }
   },
   "cell_type": "code",
   "source": "from tqdm.auto import tqdm",
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SuwUSxxFJngv",
    "ExecuteTime": {
     "end_time": "2025-06-01T23:25:55.524602Z",
     "start_time": "2025-06-01T23:25:55.522674Z"
    }
   },
   "source": [
    "# Using preptrained model to extract images features and building dict where key:images_names and values:images_features\n",
    "def preprocess_image(model,images_path,images_list) :\n",
    "  features = {}\n",
    "  for img in tqdm(images_list):\n",
    "    path = os.path.join(images_path,img)\n",
    "    image = Image.open(path)\n",
    "    image = image.resize((224,224))\n",
    "    image = np.expand_dims(image, axis = 0)\n",
    "    image = image / 127.5\n",
    "    image = image -1\n",
    "    # dont print status bar\n",
    "    feature = model.predict(image, verbose = 0)\n",
    "    features[img] = feature\n",
    "  return features"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-06-01T23:25:55.533288Z"
    }
   },
   "source": [
    "features = preprocess_image(features_extractor,images_path,images_names)\n",
    "path = Path.cwd() / 'images_features.bin'\n",
    "\n",
    "# saving images_features dict into .bin file\n",
    "pickle.dump(features,open(path,'wb'))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/31785 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0a53b3bb123040f08cd0c3caae9040a5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-02 01:25:55.634564: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# features_extractor.save(\"features_extractor.h5\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "zzwFMKrXELHT"
   },
   "source": [
    "## ***5-Loading prepared files***"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "X1_eGTp71ErC"
   },
   "source": [
    "# adding <start> and <end> to each caption\n",
    "def load_tokens(path,images) :\n",
    "  lines = read_file(path)\n",
    "  tokens = {}\n",
    "  for line in lines :\n",
    "    img , caption = line.split('\\t')\n",
    "    if img in images :\n",
    "      if img not in tokens.keys() :\n",
    "        tokens[img] = []\n",
    "      tokens[img].append(\"<start> \"+caption+\" <end>\")\n",
    "  return tokens"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# listing all available images\n",
    "def list_images(path) :\n",
    "   all_images = []\n",
    "   lines = read_file(path)\n",
    "   for line in lines :\n",
    "    img , caption = line.split('\\t')\n",
    "    if img not in all_images :\n",
    "       all_images.append(img)\n",
    "   return all_images\n",
    "\n",
    "all_images_list = list_images(\"E:\\Khaled\\Data\\Projects\\Image Caption Generator - GPU\\cleaned_data.txt\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***6-Splitting Images into Training, Validation & Testing sets***"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# training_images , testing_images = train_test_split(all_images_list , test_size = .1 , shuffle = True)\n",
    "# cross_validation_images , testing_images = train_test_split(testing_images , test_size = .5 , shuffle = True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def write_list_to_file(input_list, file_name):\n",
    "    with open(file_name, \"w\") as file:\n",
    "        for item in input_list:\n",
    "            file.write(str(item) + \"\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# write_list_to_file(training_images, 'training_images_list.txt')\n",
    "# write_list_to_file(cross_validation_images, 'cross_validation_images_list.txt')\n",
    "# write_list_to_file(testing_images, 'testing_images_list.txt')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ihiI6YP3idv8"
   },
   "source": [
    "import pickle\n",
    "training_images = read_file(r\"E:\\Khaled\\Data\\Projects\\Image Caption Generator - GPU\\training_images_list.txt\")\n",
    "# loading training images_captions dict\n",
    "training_tokens = load_tokens(r\"E:\\Khaled\\Data\\Projects\\Image Caption Generator - GPU\\cleaned_data.txt\",training_images)\n",
    "# loading extracted images features\n",
    "features = pickle.load(open(r\"E:\\Khaled\\Data\\Projects\\Image Caption Generator - GPU\\images_features.bin\",'rb'))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "D3D6IeiRFiUv"
   },
   "source": [
    "## ***7-Building text vectorization model***"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "f5RJcxTS6Bet"
   },
   "source": [
    "# extracting all captions into one list\n",
    "def fetch_captions(tokens) :\n",
    "  captions = []\n",
    "  for caps in tokens.values() :\n",
    "    [captions.append(cap) for cap in caps]\n",
    "  return captions"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fcGGrLFjgEZw"
   },
   "source": [
    "captions = fetch_captions(training_tokens)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6lYt7LRgNFYC"
   },
   "source": [
    "sentences_length = []\n",
    "for caption in captions :\n",
    "  sentences_length.append(len(caption.split()))\n",
    "\n",
    "max_length = max(sentences_length)\n",
    "full_text = ' '.join(captions)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "galqH6Q2NyqI"
   },
   "source": [
    "text_dataset = tf.data.Dataset.from_tensor_slices(captions)\n",
    "\n",
    "# preparing TextVectorization layer to be used to tokenize captions\n",
    "vectorize_layer = TextVectorization(output_mode = 'int' )\n",
    "vectorize_layer.adapt(text_dataset)\n",
    "\n",
    "# building vocab using TextVectorization layer\n",
    "vocabulary = list(vectorize_layer.get_vocabulary())\n",
    "vocab_size = vectorize_layer.vocabulary_size()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Pickle the config and weights\n",
    "pickle.dump({'config': vectorize_layer.get_config(),'weights': vectorize_layer.get_weights()}, open(\"tv_layer.pkl\", \"wb\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# tokenizing captions and saving it back to dict where keys:images and values:sequences\n",
    "training_images_sequences = {}\n",
    "i = 0\n",
    "for img , captions in training_tokens.items():\n",
    "    training_images_sequences[img] = []\n",
    "    for caption in captions :\n",
    "        sequence =  vectorize_layer(tf.constant([caption])).numpy().tolist()[0]\n",
    "        training_images_sequences[img].append(sequence)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# with open('training_images_sequences.json', 'w') as f:\n",
    "#     # Serialize the dictionary to JSON and write it to the file\n",
    "#     json.dump(training_images_sequences, f)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with open(r\"E:\\Khaled\\Data\\Projects\\Image Caption Generator - GPU\\training_images_sequences.json\", 'r') as f:\n",
    "    # Load the JSON data from the file and deserialize it to a Python object\n",
    "    training_tokens = json.load(f)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "tTwzjddbETj7"
   },
   "source": [
    "## ***8-Building data generator***"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def data_generator(tokens_keys,tokens,features,vocab_size,max_length,batch_size) :\n",
    "    input_1 , input_2 , output = [] , [] , []\n",
    "    n = 0\n",
    "    while 1 :\n",
    "        for img in tokens_keys :\n",
    "            sequences = tokens[img]\n",
    "            n += 1\n",
    "            if img in features.keys() :\n",
    "                feature = features[img][0]\n",
    "                for sequence in sequences :\n",
    "                    for index in range(1,len(sequence)) :\n",
    "                        input_b = sequence[:index]\n",
    "                        input_b = pad_sequences([input_b], maxlen = max_length, padding='post')[0]\n",
    "                        output_w = sequence[index]\n",
    "                        output_w = to_categorical([output_w],num_classes=vocab_size)[0]\n",
    "                        input_1.append(feature)\n",
    "                        input_2.append(input_b)\n",
    "                        output.append(output_w)\n",
    "            \n",
    "            if n == batch_size :\n",
    "                try :\n",
    "                    input_1, input_2 , output = np.array(input_1), np.array(input_2), np.array(output)\n",
    "                    yield [input_1,input_2],output\n",
    "                    input_1 , input_2 , output = [] , [] , []\n",
    "                    n = 0\n",
    "                except :\n",
    "                    print(\"Skipped\")\n",
    "                    input_1 , input_2 , output = [] , [] , []\n",
    "                    n = 0"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "7X693hpUEeMm"
   },
   "source": [
    "## ***9-Building Captioning Model***"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "no_of_features = 4096\n",
    "  \n",
    "def build_model(no_of_features,max_length,output_size,learning_rate) :\n",
    "\n",
    "# images features model path\n",
    "  input_img = Input(shape=(no_of_features,))\n",
    "  cnn_layer1 = Dropout(.4)(input_img)\n",
    "  cnn_layer2 = Dense(256, activation = 'relu')(cnn_layer1)\n",
    "  \n",
    "# sequences path\n",
    "  input_seq = Input(shape=(max_length,))\n",
    "  lstm_layer1 = Embedding(output_size,300,input_length = max_length , mask_zero = True )(input_seq)\n",
    "  lstm_layer2 = Dropout(.4) (lstm_layer1)\n",
    "  lstm_layer3 = LSTM(256,activation='tanh') (lstm_layer2)\n",
    "\n",
    "  merging_layer = add([cnn_layer2,lstm_layer3])\n",
    "  final_dense = Dense(256 , activation ='relu')(merging_layer)\n",
    "  output = Dense(output_size , activation ='softmax')(final_dense)\n",
    "\n",
    "  model = Model(inputs = [input_img,input_seq] , outputs = output )\n",
    "\n",
    "  optimizer = Adam(learning_rate=learning_rate)\n",
    "  model.compile(loss = 'categorical_crossentropy' , optimizer = optimizer)\n",
    "\n",
    "  return model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8DTk-nSrNHLj"
   },
   "source": [
    "captioning_model = build_model(no_of_features,max_length,vocab_size, .001)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 754
    },
    "id": "-Vb3mIMaJ2Ug",
    "outputId": "90ec0d4d-50f1-4956-ad56-20e99e981e7c"
   },
   "source": [
    "plot_model(captioning_model , show_shapes = True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hFsSXQKfjrbJ",
    "outputId": "387459ac-2012-4d38-f2b2-e7867f1fd949"
   },
   "source": [
    "captioning_model.summary()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Gko-6hBXFH20"
   },
   "source": [
    "## ***10-Checking GPU Power***"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# making sure of the GPU power\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(tf.config.experimental.list_physical_devices('GPU'))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "tf.config.experimental.set_visible_devices(tf.config.experimental.list_physical_devices('GPU')[0], 'GPU')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***11-Model Training***"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define a custom callback class to track the training loss history\n",
    "class LossHistory(tf.keras.callbacks.Callback):\n",
    "\n",
    "# Define a function to initialize the loss history list at the beginning of training\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "    \n",
    "# Define a function to append the training loss at the end of each epoch\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "\n",
    "history = LossHistory()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model_loss = []"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O4h9kJpfYC1e",
    "outputId": "98ba7f62-2c65-478d-c126-05b302db0404"
   },
   "source": [
    "steps = len(training_tokens) / 64\n",
    "\n",
    "for i in range(50) :\n",
    "  # shuffling training data before each epoch\n",
    "  tokens_keys = list(training_tokens.keys())\n",
    "  random.shuffle(tokens_keys)\n",
    "  data = data_generator(tokens_keys,training_tokens,features,vocab_size,max_length,64)\n",
    "\n",
    "  captioning_model.fit(data , epochs = 1 , steps_per_epoch=steps , verbose =1, callbacks=[history])\n",
    "\n",
    "  # extracting epoch model loss and saving it into txt file\n",
    "  loss = history.losses\n",
    "  model_loss.append(loss[0])\n",
    "  write_list_to_file(model_loss, 'model_loss.txt')\n",
    "\n",
    "captioning_model.save('model_49.h5')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-sSEQs8HHZkP"
   },
   "source": [
    "# loading txt file into list\n",
    "def read_file_to_list(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    return [line.strip() for line in lines]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lUDl_UFPFimb"
   },
   "source": [
    "model_loss = read_file_to_list(r\"E:\\Khaled\\Data\\Projects\\Image Caption Generator - GPU\\saved_models\\model_loss.txt\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***12-Inference using greedy algorithm***"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "captioning_model = tf.keras.models.load_model(r\"E:\\Khaled\\Data\\Projects\\Image Caption Generator - GPU\\saved_models\\model_49.h5\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# importing saved TextVectorization layer\n",
    "from_disk = pickle.load(open(\"tv_layer.pkl\", \"rb\"))\n",
    "vectorize_layer = TextVectorization.from_config(from_disk['config'])\n",
    "vectorize_layer.adapt(tf.data.Dataset.from_tensor_slices([\"xyz\"]))\n",
    "vectorize_layer.set_weights(from_disk['weights'])\n",
    "\n",
    "# restoring vocab using TextVectorization layer\n",
    "vocabulary = list(vectorize_layer.get_vocabulary())\n",
    "vocab_size = vectorize_layer.vocabulary_size()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# loading saved fatures exctractor model\n",
    "features_extractor = tf.keras.models.load_model(r\"E:\\Khaled\\Data\\Projects\\Image Caption Generator - GPU\\saved_models\\features_extractor.h5\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def get_features_from_image(image_path,model) :\n",
    "  img = Image.open(image_path)\n",
    "  img = img.resize((224,224))\n",
    "  img = np.expand_dims(img,axis = 0)\n",
    "  img = img/127.5\n",
    "  img = img-1\n",
    "  features = model.predict(img)\n",
    "  return features\n",
    "\n",
    "def get_word(index,vocab) :\n",
    "  word = vocab[index]\n",
    "  return word"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DaTCU0SbDFkk"
   },
   "source": [
    "def get_caption(path,features_extractor,vectorize_layer,captioning_model):\n",
    "  my_features = get_features_from_image(path,features_extractor)\n",
    "  caption = '<start>'\n",
    "  for i in range(max_length) :\n",
    "    sequenced_caption = vectorize_layer(tf.constant([caption])).numpy().tolist()\n",
    "    padded_sequenced_caption = pad_sequences(sequenced_caption , maxlen = max_length, padding='post')[0]\n",
    "    padded_sequenced_caption = np.resize(padded_sequenced_caption,(1,max_length))\n",
    "    output = captioning_model.predict([my_features , padded_sequenced_caption])\n",
    "    index = np.argmax(output)\n",
    "    if index == 2 :\n",
    "      caption = caption + ' <end>'\n",
    "      return caption\n",
    "    else :\n",
    "      current_word = get_word(index,vocabulary)\n",
    "      caption = caption + ' ' + current_word\n",
    "  return caption"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def get_caption_show_image(image_path=None) :\n",
    "    if image_path is not None :\n",
    "        image_path = image_path\n",
    "    else :\n",
    "        images_names = read_file(r\"E:\\Khaled\\Data\\Projects\\Image Caption Generator - GPU\\testing_images_list.txt\")\n",
    "        image_index = random.randint(0,len(images_names))\n",
    "        image_path = os.path.join(r\"E:\\Khaled\\Data\\Projects\\Image Caption Generator - GPU\\flickr30k_images\",images_names[image_index])\n",
    "\n",
    "    caption = get_caption(image_path,features_extractor,vectorize_layer,captioning_model)[8:-5]\n",
    "    print(caption)\n",
    "    image = plt.imread(image_path)\n",
    "    fig,ax = plt.subplots()\n",
    "    ax.imshow(image)\n",
    "    ax.axis('off')\n",
    "\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***12.1-Testing using random Images from testing set***"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "get_caption_show_image()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "get_caption_show_image()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "get_caption_show_image()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "get_caption_show_image()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "get_caption_show_image()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***12.2-Testing using Images downloaded from web***"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "get_caption_show_image(r\"C:\\Users\\User\\Desktop\\pexels-chevanon-photography-1108099.jpg\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "get_caption_show_image(r\"C:\\Users\\User\\Desktop\\pexels-pixabay-2209.jpg\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "get_caption_show_image(r\"C:\\Users\\User\\Desktop\\pexels-pixabay-2346.jpg\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "get_caption_show_image(r\"C:\\Users\\User\\Desktop\\pexels-pixabay-248547.jpg\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "get_caption_show_image(r\"C:\\Users\\User\\Desktop\\pexels-milena-de-narvaez-ayllon-2889030.jpg\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***13-Inference using beam search algorithm***"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# getting top k probabilities and indexes\n",
    "def get_word_preds(sentence,testing_image,beam_size) :\n",
    "    sequenced_caption = vectorize_layer(tf.constant([sentence])).numpy().tolist()\n",
    "    padded_sequenced_caption = pad_sequences(sequenced_caption , maxlen = max_length, padding='post')[0]\n",
    "    padded_sequenced_caption = np.resize(padded_sequenced_caption,(1,max_length))\n",
    "    preds = captioning_model.predict([testing_image , padded_sequenced_caption])\n",
    "    word_preds_indexes = np.argsort(preds[0])[-beam_size:]\n",
    "    return preds,word_preds_indexes"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# getting top k captions using beam search algorithm\n",
    "def get_caption_with_beam(image_path,beam_size) :\n",
    "\n",
    "    # extracting image feature\n",
    "    my_testing_image = get_features_from_image(image_path,features_extractor)\n",
    "\n",
    "    cap = \"<start>\"\n",
    "    current_k_sentences = {}\n",
    "    final_captions = []\n",
    "    new_hypotheses = []\n",
    "\n",
    "   \n",
    "    preds,word_preds_indexes = get_word_preds(cap,my_testing_image,beam_size)\n",
    "\n",
    "    \n",
    "    for w in word_preds_indexes:\n",
    "        new_seq = [cap]\n",
    "        new_seq.append(get_word(w,vocabulary))\n",
    "        new_hypotheses.append((new_seq, preds[0][w]))\n",
    "\n",
    "    \n",
    "    new_hypotheses = sorted(new_hypotheses, key=lambda x: x[1], reverse=True)[:beam_size]\n",
    "    for seq, prob in new_hypotheses:\n",
    "        current_k_sentences[\" \".join(seq)] = prob\n",
    "\n",
    "    for _ in range(max_length-1) :\n",
    "        all_sentences = []\n",
    "        \n",
    "        for sentence in current_k_sentences.keys() :\n",
    "            # making sure the caption stops at \"end\" \n",
    "            if sentence[-3:] == \"end\" :\n",
    "                final_captions.append((sentence,current_k_sentences[sentence]))\n",
    "                beam_size = beam_size-1\n",
    "                if beam_size == 0 :\n",
    "                    break\n",
    "                continue\n",
    "            \n",
    "            preds,word_preds_indexes = get_word_preds(sentence,my_testing_image,beam_size)\n",
    "\n",
    "            new_hypotheses = []\n",
    "        \n",
    "            for w in word_preds_indexes:\n",
    "                new_seq = [sentence]\n",
    "                new_seq.append(get_word(w,vocabulary))\n",
    "                new_hypotheses.append((new_seq, current_k_sentences[sentence]*preds[0][w]))\n",
    "\n",
    "            new_hypotheses = sorted(new_hypotheses, key=lambda x: x[1], reverse=True)[:beam_size]\n",
    "            \n",
    "            for seq, prob in new_hypotheses:\n",
    "                all_sentences.append((\" \".join(seq),prob))\n",
    "\n",
    "        \n",
    "        all_sentences = sorted(all_sentences, key=lambda x: x[1], reverse=True)[:beam_size]\n",
    "        current_k_sentences = {}\n",
    "        for seq, prob in all_sentences:\n",
    "            current_k_sentences[\"\".join(seq)] = prob\n",
    "\n",
    "        if beam_size == 0 :\n",
    "            break\n",
    "    \n",
    "    # printing top k captions\n",
    "    final_captions = sorted(final_captions, key=lambda x: x[1], reverse=True)\n",
    "    for cap,prob in final_captions :\n",
    "        print(cap[8:-4])\n",
    "    \n",
    "    # displaying testing image\n",
    "    image = plt.imread(image_path)\n",
    "    fig,ax = plt.subplots()\n",
    "    ax.imshow(image)\n",
    "    ax.axis('off')\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "final_captions = get_caption_with_beam(r\"C:\\Users\\User\\Desktop\\Kids-now-spend-twice-as-much-time-playing-indoors-than-outdoors.jpg\",6)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "final_captions = get_caption_with_beam(r\"C:\\Users\\User\\Desktop\\pexels-tarikul-raana-3619972.jpg\",6)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***14-Visualizing Model Performance***"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model_loss = [round(float(value),3) for value in model_loss ]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Gy7OvkE1MG2N"
   },
   "source": [
    "fig , ax = plt.subplots()\n",
    "ax.plot(range(len(model_loss)),model_loss)\n",
    "ax.set_xlabel(\"No of epochs\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_title(\"Training Loss\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***15-BLEU Score***"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "def evaluation_func(captioning_model,vectorize_layer,features_extractor,images_folder,images_set_path) :\n",
    "  images_names = read_file(images_set_path)\n",
    "  images_tokens = load_tokens(r\"E:\\Khaled\\Data\\Projects\\Image Caption Generator - GPU\\cleaned_data.txt\",images_names)\n",
    "  actual , predicted = list() , list()\n",
    "  for image in images_tokens.keys() :\n",
    "    image_path = os.path.join(images_folder,image)\n",
    "    generated_caption = get_caption(image_path,features_extractor,vectorize_layer,captioning_model)\n",
    "    actual_captions = images_tokens[image]\n",
    "    actual.append([caption.split() for caption in actual_captions])\n",
    "    predicted.append(generated_caption.split())\n",
    "\n",
    "  BLEU_2 = corpus_bleu(actual,predicted,weights=(0.5, 0.5, 0, 0))\n",
    "  return BLEU_2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "training_bleu_score  = evaluation_func(captioning_model,vectorize_layer,features_extractor,r\"E:\\Khaled\\Data\\Projects\\Image Caption Generator - GPU\\flickr30k_images\",r\"E:\\Khaled\\Data\\Projects\\Image Caption Generator - GPU\\training_images_list.txt\")\n",
    "validation_bleu_score = evaluation_func(captioning_model,vectorize_layer,features_extractor,r\"E:\\Khaled\\Data\\Projects\\Image Caption Generator - GPU\\flickr30k_images\",r\"E:\\Khaled\\Data\\Projects\\Image Caption Generator - GPU\\cross_validation_images_list.txt\")\n",
    "testing_bleu_score  = evaluation_func(captioning_model,vectorize_layer,features_extractor,r\"E:\\Khaled\\Data\\Projects\\Image Caption Generator - GPU\\flickr30k_images\",r\"E:\\Khaled\\Data\\Projects\\Image Caption Generator - GPU\\testing_images_list.txt\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(f\"Training BLEU Score : {training_bleu_score}\")\n",
    "print(f\"Cross Validation BLEU Score : {validation_bleu_score}\")\n",
    "print(f\"Testing BLEU Score : {testing_bleu_score}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "d2SQL5p2cCbN"
   },
   "source": [
    "It's important to note that the BLEU score is not the only evaluation metric used for image captioning models, and that other metrics such as METEOR, ROUGE, and CIDEr may also be used to evaluate a model's performance.\n",
    "It's also worth mentioning that BLEU is a controversial evaluation metric for image captioning, as it only measures the overlap between the generated captions and the reference captions, and does not take into account other factors such as the fluency, coherence, and overall quality of the captions."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "caption",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "1bcdc58371a91ec52825c51d6513b8353d7b4aeca9910996b1173e1349d67aa8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
